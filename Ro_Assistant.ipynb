{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b450cb98",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install pyaudio faster-whisper ollama pyttsx3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96675dad",
   "metadata": {},
   "source": [
    "## Complete Offline Voice Assistant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "import ollama\n",
    "import pyttsx3\n",
    "import time\n",
    "\n",
    "class OfflineAssistant:\n",
    "    def __init__(self):\n",
    "        # Audio config\n",
    "        self.CHUNK = 1024\n",
    "        self.FORMAT = pyaudio.paInt16\n",
    "        self.CHANNELS = 1\n",
    "        self.RATE = 16000\n",
    "        self.SILENCE_LIMIT = 2  # Seconds of silence to stop recording\n",
    "        \n",
    "        # Initialize models\n",
    "        print(\"Loading Whisper...\")\n",
    "        self.stt_model = WhisperModel(\"base.en\", device=\"\")\n",
    "        print(\"Loading TTS...\")\n",
    "        self.tts = pyttsx3.init()\n",
    "        self._setup_audio()\n",
    "\n",
    "    def _setup_audio(self):\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.stream = self.p.open(\n",
    "            format=self.FORMAT,\n",
    "            channels=self.CHANNELS,\n",
    "            rate=self.RATE,\n",
    "            input=True,\n",
    "            frames_per_buffer=self.CHUNK\n",
    "        )\n",
    "\n",
    "    def record_voice(self):\n",
    "        \"\"\"Record until silence is detected\"\"\"\n",
    "        print(\"\\nSpeak now...\")\n",
    "        frames = []\n",
    "        silent_frames = 0\n",
    "        silence_threshold = 500  # Adjust based on your mic\n",
    "        \n",
    "        while True:\n",
    "            data = self.stream.read(self.CHUNK, exception_on_overflow=False)\n",
    "            audio_data = np.frombuffer(data, dtype=np.int16)\n",
    "            \n",
    "            # Check for silence\n",
    "            if np.abs(audio_data).mean() < silence_threshold:\n",
    "                silent_frames += 1\n",
    "                if silent_frames > self.SILENCE_LIMIT * (self.RATE/self.CHUNK):\n",
    "                    break\n",
    "            else:\n",
    "                silent_frames = 0\n",
    "                frames.append(data)\n",
    "        \n",
    "        return b''.join(frames)\n",
    "\n",
    "    def transcribe(self, audio_bytes):\n",
    "        \"\"\"Convert speech to text\"\"\"\n",
    "        with wave.open(\"temp.wav\", \"wb\") as wf:\n",
    "            wf.setnchannels(self.CHANNELS)\n",
    "            wf.setsampwidth(2)  # 16-bit = 2 bytes\n",
    "            wf.setframerate(self.RATE)\n",
    "            wf.writeframes(audio_bytes)\n",
    "        \n",
    "        segments, _ = self.stt_model.transcribe(\"temp.wav\")\n",
    "        return \" \".join(segment.text for segment in segments).strip()\n",
    "\n",
    "    def respond(self, text):\n",
    "        \"\"\"Get AI response and speak\"\"\"\n",
    "        if not text:\n",
    "            return \"I didn't hear that\"\n",
    "            \n",
    "        if any(cmd in text.lower() for cmd in [\"exit\", \"quit\", \"bye\"]):\n",
    "            return \"Goodbye!\"\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            model='mistral',\n",
    "            messages=[{'role': 'user', 'content': text}]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Ro Assistant Ready! (Say 'exit' to quit)\")\n",
    "        try:\n",
    "            while True:\n",
    "                # Record\n",
    "                audio = self.record_voice()\n",
    "                \n",
    "                # Transcribe\n",
    "                text = self.transcribe(audio)\n",
    "                if not text:\n",
    "                    print(\"No speech detected\")\n",
    "                    continue\n",
    "                    \n",
    "                print(f\"You: {text}\")\n",
    "                \n",
    "                # Respond\n",
    "                reply = self.respond(text)\n",
    "                print(f\"Ro: {reply}\")\n",
    "                self.tts.say(reply)\n",
    "                self.tts.runAndWait()\n",
    "                \n",
    "                if \"goodbye\" in reply.lower():\n",
    "                    break\n",
    "                    \n",
    "        finally:\n",
    "            self.stream.stop_stream()\n",
    "            self.stream.close()\n",
    "            self.p.terminate()\n",
    "\n",
    "# Start the assistant\n",
    "assistant = OfflineAssistant()\n",
    "assistant.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
